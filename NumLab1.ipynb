{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerisk laboration I: Neurala nätverk\n",
    "\n",
    "## Bayesiansk inferens och maskininlärning [TIF385], Chalmers, lp2 2024\n",
    "\n",
    "Senast uppdaterad: 15-Okt-2024 av Christian Forssén [christian.forssen@chalmers.se]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruktioner\n",
    "- Se deadline för examination på kurshemsidan.\n",
    "- Denna numeriska laboration examineras genom individuellt godkänt på obligatoriska uppgifter i Yata, samt genom godkänd redovisning av lösningar och resultat vid en gruppdiskussion som leds av övningsledare.\n",
    "- Laborationen genomförs individuellt men redovisning sker i seminarieform med flera studenter åt gången.\n",
    "- Diskussioner och samarbete mellan studenter är tillåtet, men varje student som deltar i en redovisning måste på egen hand kunna svara på frågor kring lösning och tolkning av samtliga delar av laborationen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultat och figurer\n",
    "- Vid gruppredovisningen skall varje student vara beredd att köra sin lösningskod och kunna demonstrera resultat via efterfrågade figurer.\n",
    "- Alla figurer skall vara tydliga att avläsa, med rubriker på axlar och en beskrivning av kurvor och/eller data.\n",
    "- Ni skall kunna tolka era resultat och förklara hur dessa genererades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uppgift\n",
    "\n",
    "Det övergripande målet är konstruera ett neuralt nätverk i Python. För storskaliga tillämpningar använder man oftast optimerade programbibliotek som [`tensorflow`](https://www.tensorflow.org/) med omfattande funktionalitet. I denna laboration skall du färdigställa en egen implementering för att tydligare kunna förstå och arbeta med de olika komponenterna. \n",
    "\n",
    "Slutmålet är att skapa en Pythonklass för ett neuralt nätverk med ett dolt lager för att sedan initialisera en instans av denna klass som tränas för att kunna \"lösa\" Schrödingerekvationen för en kvantmekanisk partikel i olika potentiallådor i en dimension.\n",
    "\n",
    "Mer specifikt så utförs uppgiften i tre steg som beskrivs närmare nedan:\n",
    "\n",
    "* **Steg 1: Grundfunktionalitet som sedan skall integreras i vår `Neural_Network` python-klass**\n",
    "* **Steg 2: Bygg klart vår `Neural_Network` python-klass**\n",
    "* **Steg 3: Skapa och träna ett neuralt nätverk att lösa Schrödingerekvationen**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lärandemål\n",
    "- att förstå och numeriskt implementera flera grundläggande algoritmer som används inom maskininlärning; specifikt betraktas enkla neurala nätverk.\n",
    "- att använda python för praktisk tillämpning av maskininlärning. \n",
    "- att konstruera och utföra en träning av ett neuralt nätverk för simulering av en fysikaliskt relevant differentialekvation.\n",
    "- att kunna beskriva sin kod och sina resultat på ett effektivt och tydligt sätt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modulimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import optimize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steg 1 (Grundfunktionalitet som sedan skall integreras i vår `Neural_Network` python-klass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Här kommer vi att betrakta ett neuralt nätverk med två insignaler $(x_1, x_2)$, ett dolt lager med 100 noder och sigmoid-aktivering, och en utsignal $y$ som är linjär i utsignalsnodens aktivering.\n",
    "\n",
    "Ett sådant neuralt nätverk har på förhand tränats för att kunna reproducera\n",
    "\n",
    "$$\n",
    "y = x_2 / x_1,\n",
    "$$\n",
    "\n",
    "med $x_2 \\in [0,1]$ och $x_1 \\in [1,2]$. Notera att sambandet $y = y(x_1,x_2)$ återskapas genom flexibiliteten hos det neurala nätverket. Modellen vet ingenting om division. Det färdigtränande nätverkets vikter finns i datafiler som laddas in nedan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Examineras i Yata.\n",
    "\n",
    "    Bygg klart nedan aktiveringsfunktion samt dess derivata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    '''\n",
    "    Apply sigmoid activation function to scalar, vector, or matrix.\n",
    "    '''\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def sigmoidPrime(z):\n",
    "    \"\"\"\n",
    "    Gradient of sigmoid with respect to z\n",
    "    \"\"\"\n",
    "    return np.exp(-z)/(1+np.exp(-z))**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Examineras i Yata\n",
    "\n",
    "    Skapa funktionen `forward` som propagerar Nd instanser med insignaler `[x1,x2]` genom det specifika neurala nätverket. Se specifika designinstruktioner i funktionens docstring.\n",
    "    \n",
    "    Testa att funktionen fungerar med de inladdade vikterna så att\n",
    "    \n",
    "    `forward(X)`\n",
    "    \n",
    "    (förhoppningsvis) returnerar en (Nd,1) array med kvoter x2/x1 där `[x1,x2]` är rader i argumentet `X`, vilket alltså i sin tur är en (Nd,2) array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tips\n",
    "- Fundera först (gärna med penna och papper) vad det är för beräkningar som skall utföras. Det är sällan en god idé att börja med kodandet innan man har klart för sig vad som skall implementeras.\n",
    "- Se gärna relevanta avsnitt i kurskompendiet.\n",
    "- Använd `numpy` matris- och vektoroperationer istället för `for` loopar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00023172]\n"
     ]
    }
   ],
   "source": [
    "# Färdigtränade vikter som skall användas i funktionen nedan.\n",
    "# Notera att vi inte använder några biasvikter i detta exempel.\n",
    "\n",
    "# Define weight matrices W1 (2,100) and W2 (100,1)\n",
    "W1 = np.load('W1_step1.npy')\n",
    "W2 = np.load('W2_step1.npy')\n",
    "\n",
    "def forward(X):\n",
    "    '''\n",
    "    Propagate X through a NN to produce an output yhat.\n",
    "    \n",
    "    Args:\n",
    "        X: ndarray (Ndata, 2)\n",
    "            Input data\n",
    "        \n",
    "    Returns:\n",
    "        yhat: ndarray (Ndata, 1)\n",
    "            Response\n",
    "            \n",
    "    Input layer: 2 inputs\n",
    "    Hidden layer: 100 neurons (weights in W1, shape (2,100))\n",
    "    Output layer: 1 output (weights in W2, shape (100,1))\n",
    "        \n",
    "    The activation of neuron j in the hidden layer is*\n",
    "    \n",
    "    z1[j] = X[0] * W1[0,j] + X[1] * W1[1,j]\n",
    "    \n",
    "    and we use a sigmoid activation function\n",
    "    \n",
    "    y1 = f(z1) = 1 / (1 + exp(-z1))\n",
    "    \n",
    "    The activation of the output neutron is*\n",
    "    \n",
    "    z2 = sum_k ( y1[k] * W2[k,0] )\n",
    "    \n",
    "    and we use a linear output in the output layer\n",
    "    \n",
    "    yhat = z2\n",
    "    \n",
    "    Note that there is no bias in this example\n",
    "    '''\n",
    "    \n",
    "    z1 = np.matmul(X,W1)\n",
    "    y1 = sigmoid(z1)\n",
    "    z2 = np.matmul(y1,W2)\n",
    "    yhat = z2\n",
    "    \n",
    "    return yhat\n",
    "print(forward([1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steg 2 (Bygg klart vår `Neural_Network` python-klass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Examineras i Yata\n",
    "  \n",
    "    Lägg till följande funktionalitet till nedan python class `Neural_Network`:\n",
    "    - funktionerna `sigmoid` och `sigmoidPrime` från Steg 1 (men anpassade för att vara metoder i en python-klass)\n",
    "    - funktionen `forward` från Steg 1 (men anpassad för att vara en metod i en python-klass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Examineras i Yata\n",
    "  \n",
    "    Lägg till följande funktionalitet till nedan python class `trainer`:\n",
    "    - funktionen `score` som kan beräkna ett MSE (mean-squared error) felmått för given data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kort sammanfattning av det du behöver veta om python-klasser:\n",
    "- En **klass** innehåller funktionalitet som man vill kunna återanvända. Nedan finner vi klasserna `Neural_Network` som låter oss definiera olika versioner av ett neuralt nätverk med ett dolt lager, samt `trainer` som låter oss träna ett neuralt nätverk.\n",
    "- Specifikt kan man skapa flera **instanser** av en klass, där varje instans kan betraktas som en egen kopia med sina egna egenskaper (attribut).\n",
    "- Inuti en klass kan man definiera olika **attribut** (variabler) och **metoder** (funktioner) som sedan kan utnyttjas internt i klassen genom anrop `self.var` för en variabel `var`, eller `self.func()` för en funktion `func`.\n",
    "- Dessa attribut kan till exempel initialiseras vid det första anropet. Säg till exempel att vi skapar instansen\n",
    "\n",
    "  `NN = Neural_Network(insize=2,outsize=1,hiddensize=100, output='linear')`\n",
    "  \n",
    "  Då finner vi att `NN.outputLayerSize` returnerar 1 (se initialiseringsmetoden `__init__` nedan).\n",
    "- Metoder som definieras inne i en klass måste ha `self` som sitt första argument.\n",
    "- Metoden `__init__` är speciell då den alltid körs när en instans av en klass skapas. Här brukar man initialisera viktiga attribut som beskriver just denna instans av klassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    \n",
    "    def __init__(self, insize=2, outsize=1, hiddensize=3, output='linear'):\n",
    "        #Define Hyperparameters\n",
    "        self.inputLayerSize = insize\n",
    "        self.outputLayerSize = outsize\n",
    "        self.hiddenLayerSize = hiddensize\n",
    "        if output=='linear':\n",
    "            self.output = self.linear\n",
    "            self.outputPrime = self.linearPrime\n",
    "        elif output=='sigmoid':\n",
    "            self.output = self.sigmoid\n",
    "            self.outputPrime = self.sigmoidPrime\n",
    "\n",
    "        # Initialize weights (parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Propogate inputs through network\n",
    "        \n",
    "        Note: \n",
    "        refer to weight arrays as \n",
    "        self.W1\n",
    "        self.W2\n",
    "        \n",
    "        and the hidden layer output\n",
    "        self.y1\n",
    "        \n",
    "        and activations\n",
    "        self.z1\n",
    "        self.z2       \n",
    "        \"\"\"\n",
    "        \n",
    "        self.z1 = np.matmul(X,self.W1)\n",
    "        self.y1 = self.sigmoid(self.z1)\n",
    "        self.z2 = np.matmul(self.y1,self.W2)\n",
    "        # Output...\n",
    "        # note how we employ the output method that has been defined at initialization\n",
    "        return self.output(self.z2)\n",
    "\n",
    "    def linear(self, z):\n",
    "        \"\"\"Apply linear activation function to scalar, vector, or matrix\"\"\"\n",
    "        return z\n",
    "\n",
    "    def linearPrime(self,z):\n",
    "        \"\"\"Gradient of linear with respect to z\"\"\"\n",
    "        return np.ones_like(z)\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Apply sigmoid activation function to scalar, vector, or matrix\"\"\"\n",
    "        return 1/(1+np.exp(-z))\n",
    "\n",
    "    def sigmoidPrime(self,z):\n",
    "        \"\"\"Gradient of sigmoid with respect to z\"\"\"\n",
    "        return np.exp(-z)/(1+np.exp(-z))**2\n",
    "\n",
    "    def costFunction(self, X, y):\n",
    "        \"\"\"Compute cost for given X,y, use weights already stored in class.\"\"\"\n",
    "        self.yHat = self.forward(X)\n",
    "        C = 0.5*sum((y-self.yHat)**2)\n",
    "        return C\n",
    "    \n",
    "    def delta_output(self, y):\n",
    "        return np.multiply(-(y-self.yHat), self.outputPrime(self.z2))\n",
    "\n",
    "    def costFunctionPrime(self, X, y):\n",
    "        \"\"\"Compute derivative with respect to W and W2 for a given X and y\"\"\"\n",
    "        self.yHat = self.forward(X)\n",
    "\n",
    "        # Derivative of output layer with respect to z2\n",
    "        delta2 = self.delta_output(y)\n",
    "        dCdW2 = np.dot(self.y1.T, delta2)\n",
    "\n",
    "        delta1 = np.dot(delta2, self.W2.T)*self.sigmoidPrime(self.z1)\n",
    "        dCdW1 = np.dot(X.T, delta1)\n",
    "\n",
    "        return dCdW1, dCdW2\n",
    "\n",
    "    #Helper Functions for interacting with other classes:\n",
    "    def getParams(self):\n",
    "        \"\"\"Get W1 and W2 unrolled into a single array\"\"\"\n",
    "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "        return params\n",
    "\n",
    "    def setParams(self, params):\n",
    "        \"\"\"Set W1 and W2 using a single parameter array.\"\"\"\n",
    "        W1_start = 0\n",
    "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
    "        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize , self.hiddenLayerSize)\n",
    "                            )\n",
    "        W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize\n",
    "        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n",
    "  \n",
    "    def computeGradients(self, X, y):\n",
    "        dCdW1, dCdW2 = self.costFunctionPrime(X, y)\n",
    "        return np.concatenate((dCdW1.ravel(), dCdW2.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    \n",
    "    def __init__(self, N, maxiter=200, disp=True):\n",
    "        \"\"\"\n",
    "        Initialize an object for training of a neural network.\n",
    "        \n",
    "        Args:\n",
    "            N: reference to an instance of the Neural_Network class.\n",
    "            maxiter: maximum number of iterations for the BFGS optimization (default: 200).\n",
    "            disp: verbose output from optimization (default: True)\n",
    "        \"\"\"\n",
    "        self.N = N\n",
    "        self.maxiter = maxiter\n",
    "        self.disp = disp\n",
    "\n",
    "    def callbackF(self, params):\n",
    "        self.N.setParams(params)\n",
    "        self.C.append(self.N.costFunction(self.X, self.y))\n",
    "        self.train_score.append(self.score(self.X, self.y))\n",
    "        if not (self.yval is None or self.Xval is None):\n",
    "            self.val_score.append(self.score(self.Xval, self.yval))\n",
    "\n",
    "    def costFunctionWrapper(self, params, X, y):\n",
    "        self.N.setParams(params)\n",
    "        cost = self.N.costFunction(X, y)\n",
    "        grad = self.N.computeGradients(X,y)\n",
    "        return cost, grad\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        MSE score\n",
    "        \n",
    "        Evaluate the NN output 'yhat' for input X \n",
    "        and compare with the observed output 'y'\n",
    "        \"\"\"\n",
    "        yhat = self.N.forward(X)\n",
    "        MSE = (np.matmul((y-yhat).T,(y-yhat)))/len(y)\n",
    "        return MSE[0]\n",
    "\n",
    "    def train(self, X, y, Xval=None, yval=None):\n",
    "        \"\"\"Make internal variables for the callback function\"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.Xval = Xval\n",
    "        self.yval = yval\n",
    "\n",
    "        #Make empty list to store costs:\n",
    "        self.C = []\n",
    "        self.train_score = []\n",
    "        self.val_score = []\n",
    "\n",
    "        params0 = self.N.getParams()\n",
    "\n",
    "        # Here we use an optimization method (the Broyden–Fletcher–Goldfarb–Shanno algorithm)\n",
    "        # that tries to minimize the cost function by employing gradient information returned by\n",
    "        # the `costFunctionWrapper` method defined above.\n",
    "        options = {'maxiter': self.maxiter, 'disp' : self.disp}\n",
    "        _res = optimize.minimize(self.costFunctionWrapper, params0, jac=True, method='BFGS', \\\n",
    "        args=(X, y), options=options, callback=self.callbackF)\n",
    "\n",
    "        self.N.setParams(_res.x)\n",
    "        self.optimizationResults = _res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steg 3 (Skapa och träna ett neuralt nätverk att lösa Schrödingerekvationen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Figur 1\n",
    "\n",
    "     Konstruera och träna ett neuralt nätverk enligt nedan instruktioner. Rita upp en inlärningskurva som visar MSE för träningsdata samt MSE för valideringsdata som en funktion av optimeringsiteration. \n",
    "     - Vad har ni valt för arkitektur och hur många parametrar har ert nätverk?\n",
    "     - Finns det tecken på överanpassning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Figur 2\n",
    "\n",
    "     Jämför hur ert färdigtränade neurala nätverk reproducerar de exakta energierna i träningsdata samt hur väl den reproducerar energierna i valideringsdata.\n",
    "     - Finns det tecken på snedvriden träningsdata?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schrödingerekvationen\n",
    "\n",
    "Problemet som betraktas i denna uppgift är Schrödingerekvationen för en kvantmekanisk partikel som befinner sig i en endimensionell lådpotential $V(x)$ med oändliga väggar $V(x=0) = V(x=1) = \\infty$.\n",
    "\n",
    "Partikelns tillstånd kommer att beskrivas av en vågfunktion $\\Psi(x)$ som i sin tur är en lösning till Schrödingerekvationen\n",
    "\n",
    "$$\n",
    "\\hat{H} \\Psi = E \\Psi,\n",
    "$$\n",
    "\n",
    "där $\\hat{H}$ är Hamiltonoperatorn som innehåller potentialfunktionen samt en differentialoperator vilken beskriver partikelns kinetiska energi. Vi kommer inte att beskriva denna operator närmare (det kommer i kvantfysikkursen) utan enbart konstatera att problemet motsvarar en så kallad egenekvation med (åtminstone) en lösning för en diskret egenenergi $E$. Det är denna storhet som vi söker.\n",
    "\n",
    "Den data som laddas nedan motsvarar flera olika fall där någon har löst Schrödingerekvationen för en viss potential $V(x)$ och hittat egenenergien $E$. Indata är $V_i \\equiv V(x_i)$ för 127 punkter så att $V(x_0)$ är den första punkten till höger om potentialväggen vid $x=0$ och $V(x_{126})$ är alldeles till vänster om potentialväggen vid $x=1$.\n",
    "\n",
    "Sambandet ser alltså ut som följer:\n",
    "\n",
    "$(V_0, V_1, \\ldots, V_{126})$ $\\Rightarrow$ Hamiltonoperatorn $\\Rightarrow$ Lös Schrödingerekvationen $\\Rightarrow$ $E$\n",
    "\n",
    "Vår uppgift är istället att:\n",
    "- Konstruera ett neuralt nätverk (med ett enda gömt lager) som...\n",
    "- ... givet en uppsättning med träningsdata, där `trainx` motsvarar olika potentialer och `trainy` är motsvarande egenenergier,\n",
    "- ... kan tränas så att det sedan kan förutsäga egenenergien för andra potentiallådor (som inte var med i träningsdatan)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tips\n",
    "- Fundera först på hur in- och utlagren skall se ut för att passa med problemet som skall lösas.\n",
    "- Välj sedan storlek på det gömda lagret och tänk på att det kan ta väldigt lång tid att träna nätverket om det totala antalet vikter blir för stort. Ni får gärna prova er fram, men ett par tusen vikter borde gå att hantera.\n",
    "- Initialisera sedan en instans av klassen `Neural_Network`, t.ex. med `NN = Neural_Network(...argument här...)`\n",
    "- För att kunna träna nätverket behöver ni initialisera en instans av klassen `Trainer` med referensen till ert nätverk som första argument: `T = Trainer(NN, ... ev andra argument...)`\n",
    "- Själva träningen utförs med metoden `train()`. Genom att ni implementerade metoden `score` i Steg 2 kommer MSE för träningsdata att beräknas i varje iteration av gradientstegsoptimeringen. Om ni ger valideringsdata som ett argument till `train` kommer även MSE för den att beräknas. Titta gärna på dessa mått för att uppskatta hur väl träningen fungerar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nedladdning av data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\blend\\anaconda3\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:708: OptimizeWarning: Maximum number of iterations has been exceeded.\n",
      "  res = _minimize_bfgs(fun, x0, args, jac, callback, **options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: 0.044139\n",
      "         Iterations: 200\n",
      "         Function evaluations: 208\n",
      "         Gradient evaluations: 208\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y can be no greater than 2D, but have shapes (200,) and (200, 1, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\blend\\Downloads\\expfys\\Bajen\\NumLab1.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/blend/Downloads/expfys/Bajen/NumLab1.ipynb#X35sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m T \u001b[39m=\u001b[39m Trainer(NN, maxiter\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m, disp\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/blend/Downloads/expfys/Bajen/NumLab1.ipynb#X35sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m T\u001b[39m.\u001b[39mtrain(trainx, trainy, Xval\u001b[39m=\u001b[39mvalidx, yval\u001b[39m=\u001b[39mvalidy)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/blend/Downloads/expfys/Bajen/NumLab1.ipynb#X35sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(T\u001b[39m.\u001b[39mtrain_score, label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTräningsdata\u001b[39m\u001b[39m\"\u001b[39m, color\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mblack\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/blend/Downloads/expfys/Bajen/NumLab1.ipynb#X35sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(T\u001b[39m.\u001b[39mval_score, label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mValideringsdata\u001b[39m\u001b[39m\"\u001b[39m, color\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mred\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/blend/Downloads/expfys/Bajen/NumLab1.ipynb#X35sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m\"\u001b[39m\u001b[39mInlärningsdata\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\blend\\anaconda3\\Lib\\site-packages\\matplotlib\\pyplot.py:3590\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3582\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mplot)\n\u001b[0;32m   3583\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot\u001b[39m(\n\u001b[0;32m   3584\u001b[0m     \u001b[39m*\u001b[39margs: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m ArrayLike \u001b[39m|\u001b[39m \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3588\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   3589\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[Line2D]:\n\u001b[1;32m-> 3590\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39mplot(\n\u001b[0;32m   3591\u001b[0m         \u001b[39m*\u001b[39margs,\n\u001b[0;32m   3592\u001b[0m         scalex\u001b[39m=\u001b[39mscalex,\n\u001b[0;32m   3593\u001b[0m         scaley\u001b[39m=\u001b[39mscaley,\n\u001b[0;32m   3594\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m({\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m: data} \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}),\n\u001b[0;32m   3595\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   3596\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\blend\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:1724\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1481\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1482\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1483\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1721\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1722\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1723\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[1;32m-> 1724\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[0;32m   1725\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[0;32m   1726\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32mc:\\Users\\blend\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_base.py:303\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[0;32m    302\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[1;32m--> 303\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_plot_args(\n\u001b[0;32m    304\u001b[0m     axes, this, kwargs, ambiguous_fmt_datakey\u001b[39m=\u001b[39mambiguous_fmt_datakey)\n",
      "File \u001b[1;32mc:\\Users\\blend\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_base.py:502\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y must have same first dimension, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    500\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhave shapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    501\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m--> 502\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y can be no greater than 2D, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    503\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    504\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    505\u001b[0m     x \u001b[39m=\u001b[39m x[:, np\u001b[39m.\u001b[39mnewaxis]\n",
      "\u001b[1;31mValueError\u001b[0m: x and y can be no greater than 2D, but have shapes (200,) and (200, 1, 1)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tränings- och valideringsdata till Steg 3\n",
    "trainx = np.load(\"trainx_step3.npy\")\n",
    "trainy = np.load(\"trainy_step3.npy\")\n",
    "validx = np.load(\"validx_step3.npy\")\n",
    "validy = np.load(\"validy_step3.npy\")\n",
    "\n",
    "NN = Neural_Network(insize=127, outsize=1, hiddensize=32, output=\"linear\")\n",
    "T = Trainer(NN, maxiter=200, disp=True)\n",
    "\n",
    "T.train(trainx, trainy, Xval=validx, yval=validy)\n",
    "\n",
    "plt.plot(T.train_score, label=\"Träningsdata\", color=\"black\")\n",
    "plt.plot(T.val_score, label=\"Valideringsdata\", color=\"red\")\n",
    "plt.title(\"Inlärningsdata\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "train_yhat = NN.forward(trainx)\n",
    "train_MSE = (np.matmul((trainy-train_yhat).T,(trainy-train_yhat)))/len(trainy)\n",
    "\n",
    "valid_yhat = NN.forward(validx)\n",
    "valid_MSE = (np.matmul((validy-valid_yhat).T,(validy-valid_yhat)))/len(validy)\n",
    "\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].scatter(trainy, train_yhat, label=\"Träningsdata\", color=\"black\", alpha=0.6)\n",
    "ax[0].plot([min(trainy), max(trainy)], [min(trainy), max(trainy)], \"k--\", label=\"perfekt linje\", color=\"red\")\n",
    "ax[0].set_xlabel(\"trainy\")\n",
    "ax[0].set_ylabel(\"train_yhat\")\n",
    "ax[0].legend()\n",
    "ax[0].grid()\n",
    "\n",
    "ax[1].scatter(validy, valid_yhat, label=\"Träningsdata\", color=\"black\", alpha=0.6)\n",
    "ax[1].plot([min(validy), max(validy)], [min(validy), max(validy)], \"k--\", label=\"perfekt linje\", color=\"red\")\n",
    "ax[1].set_xlabel(\"validy\")\n",
    "ax[1].set_ylabel(\"valid_yhat\")\n",
    "ax[1].legend()\n",
    "ax[1].grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "state": {
    "065d2168353641f29ca4ec30f7f110b9": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
